{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image features exercise\n",
    "\n",
    "In this exercise we will show that we can improve our classification performance by training  classifiers not on raw pixels but on features that are computed from the raw pixels.\n",
    "\n",
    "All of your work for this exercise will be done in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from camalab.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15., 12.) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=5000, num_validation=500, num_test=500):\n",
    "  # Load the raw CIFAR-10 data\n",
    "  cifar10_dir = 'camalab/datasets/cifar-10-batches-py' # you should change it to your own path, \n",
    "                                                      # or put the dataset to this path  \n",
    "    \n",
    "  X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir, 3)\n",
    "  \n",
    "  # Subsample the data\n",
    "  mask = range(num_training, num_training + num_validation)\n",
    "  X_val = X_train[mask]\n",
    "  y_val = y_train[mask]\n",
    "  mask = range(num_training)\n",
    "  X_train = X_train[mask]\n",
    "  y_train = y_train[mask]\n",
    "  mask = range(num_test)\n",
    "  X_test = X_test[mask]\n",
    "  y_test = y_test[mask]\n",
    "\n",
    "  return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print X_train.shape\n",
    "print X_val.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features\n",
    "For each image we will compute a Histogram of Oriented\n",
    "Gradients (HOG) as well as a color histogram. We form our final feature vector for each image by concatenating\n",
    "the HOG and color histogram feature vectors.\n",
    "\n",
    "Roughly speaking, HOG should capture the texture of the image while ignoring\n",
    "color information, and the color histogram represents the color of the input\n",
    "image while ignoring texture. As a result, we expect that using both together\n",
    "ought to work better than using either alone. Verifying this assumption would\n",
    "be a good thing to try for the bonus section.\n",
    "\n",
    "You should import your `hog` and `color_histogram` functions. Both operate on a single\n",
    "image and return a feature vector for that image. \n",
    "\n",
    "Your function should takes a set of images and a list of feature functions and evaluates each feature function on each image, storing the results in a matrix where each column is the concatenation of all feature vectors for a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# color_histogram\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "X_train_feats1 = None  # Use your functhon\n",
    "X_val_feats1 = None\n",
    "X_test_feats1 = None\n",
    "\n",
    "# HOG\n",
    "X_train_feats2 = None  # Use your functhon\n",
    "X_val_feats2 = None\n",
    "X_test_feats2 = None\n",
    "\n",
    "# Concatenating the HOG and color histogram feature vectors.\n",
    "X_train_feats = None  # you may use the 'np.concatenate' function\n",
    "X_val_feats = None\n",
    "X_test_feats = None\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat  \n",
    "X_val_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_val_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "# In k-NN, the bias dimension is useless. But you may need it in other classifiers.\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN on features\n",
    "Using the k-NN code developed earlier in the assignment. This should achieve better results than k-NN directly on top of raw pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from camalab.classifiers import KNearestNeighbor\n",
    "\n",
    "classifier = KNearestNeighbor()\n",
    "classifier.train(X_train_feats, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune the 'k'\n",
    "\n",
    "k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100]\n",
    "k_to_accuracies = {} # your should store the results in this dict\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# You must use the simple validation method. Because we have divided the       #\n",
    "# dataset to validation set and train set.                                     #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "for k in sorted(k_to_accuracies):\n",
    "    print 'k = %d, accuracy = %f' % (k, k_to_accuracies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate your classifier\n",
    "\n",
    "best_k = 1  # choose your best k\n",
    "\n",
    "# Compute and display the accuracy\n",
    "y_test_pred = classifier.predict(X_test_feats, k=best_k)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print test_accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
